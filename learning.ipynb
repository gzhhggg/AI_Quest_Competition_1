{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.ライブラリimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")    \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Input\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "# from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.vgg19 import VGG19\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.パラメータ設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像分類アルゴリズム\"VGG\"に関する設定\n",
    "# 入力画像サイズの高さと幅\n",
    "# ※この解像度をあげていればより良い精度が期待できた\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "TARGET_SIZE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "# 判定分類数（4分類を判定するモデルを構築し、そのモデルの判別結果を最後に良品、不良品の2分類に変換する前提）\n",
    "NB_CLASSES = 4\n",
    "# 学習時のエポック数\n",
    "# ※エポック数は多すぎると過学習の原因になる\n",
    "EPOCHS = 30\n",
    "# バッチサイズ\n",
    "BATCH_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, IMG_WIDTH, IMG_HEIGHT)\n",
    "else:\n",
    "    input_shape = (IMG_WIDTH, IMG_HEIGHT, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データとウェイトに関する設定\n",
    "train_data_dir = \"\" # 学習データ保存先にはbridge, horn, potato, regularのフォルダがあり、各フォルダ配下に画像が格納されている想定\n",
    "validation_data_dir = \"\" # 検証データ保存先にはbridge, horn, potato, regularのフォルダがあり、各フォルダ配下に画像が格納されている想定\n",
    "test_data_dir = \"\" # テストデータ保存先には画像データが格納されている想定\n",
    "weight_dir = \"\"#学習モデルの保存パス\n",
    "save_weights_path = os.path.join(weight_dir, \"weight_sanple\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.ImageNetで学習した重みをもつ画像分類モデルの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "精度が高いモデルの検証のために複数定義している  \n",
    "最終的にはVGG16を採用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16の定義\n",
    "base_model = VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_tensor=Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG19の定義\n",
    "base_model = VGG19(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_tensor=Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xceptionの定義\n",
    "base_model = Xception(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_tensor=Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力に近い層を課題に合わせて変更\n",
    "top_model = base_model.output\n",
    "top_model = Flatten(name='flatten')(top_model)\n",
    "top_model = Dense(512, activation='relu')(top_model)\n",
    "top_model = Dropout(0.5)(top_model)\n",
    "top_model = Dense(NB_CLASSES, activation='softmax')(top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再定義\n",
    "model = Model(\n",
    "    inputs=base_model.input,\n",
    "    outputs=top_model\n",
    ")\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  VGGの学習方法の定義\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.RMSprop(lr=1e-4), #optimizerの種類（\"RMSprop\"の箇所）と学習率（\"lr\"の箇所）を変更することで、精度向上が期待できる。\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワーク構造の確認\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.学習・検証データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGGに入力できるよう画像サイズの圧縮\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255) # 前処理を（）内に追加可能\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0/255) # 前処理を（）内に追加可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習・検証データの読み込み\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習・検証データとして上記にて読み込んだ画像を設定\n",
    "nb_train_samples = train_generator.samples\n",
    "nb_validation_samples = validation_generator.samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflowはCPUで処理するため学習に時間がかかる、画像枚数が増えれば増えるほどとてつもなく時間がかかる！！  \n",
    "tensorflow-gpuで学習させたら処理時間は1/1000になります  \n",
    "GPUで処理することを強くお勧めします  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples/BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples/BATCH_SIZE\n",
    ")\n",
    "model.save_weights(save_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss値は低ければ低いほど精度は良いと思うが、Traing lossだけ低くなっていてValid lossに変化がない場合は過学習している。エポック数の変更が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss curveの表示\n",
    "plt.figure(figsize=[10,8])\n",
    "plt.plot(model.history.history['loss'], 'r')\n",
    "plt.plot(model.history.history['val_loss'], 'b')\n",
    "plt.legend(['Training loss', 'Validation Loss'])\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.title('Loss Curves', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy curveの表示\n",
    "plt.figure(figsize=[10,8])\n",
    "plt.plot(model.history.history['accuracy'], 'r')\n",
    "plt.plot(model.history.history['val_accuracy'], 'b')\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.title('Accuracy Curves', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.モデルによる判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weightファイルの読み込み\n",
    "print('load model...')\n",
    "model.load_weights(save_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 良否判定実行関数\n",
    "def get_predict(model,\n",
    "                train_data_dir: str,\n",
    "                test_data_dir: str):\n",
    "    \"\"\"This function will performs model inferencing using test data\n",
    "    and stores the results into the lists.\n",
    "    \n",
    "    Args:\n",
    "        model (object): The trained model.\n",
    "        train_data_dir (str): The location of train images.\n",
    "        test_data_dir (str): The location of test images.\n",
    "        \n",
    "    Returns:\n",
    "        filenames (list): filenames of predicted images.\n",
    "        true_classes (list): true classes of predicted images.\n",
    "        pred_classes (list): prediction classes of predicted images.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "    test_generator = data_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=TARGET_SIZE,\n",
    "        class_mode=None,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    preds = model.predict_generator(test_generator)\n",
    "\n",
    "    preds_class_idx = preds.argmax(axis=-1)\n",
    "    \n",
    "    # get prediction class\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    idx_to_class = {v: k for k, v in train_generator.class_indices.items()}\n",
    "    pred_classes = np.vectorize(idx_to_class.get)(preds_class_idx)\n",
    "    filenames_to_class = list(zip(test_generator.filenames, pred_classes))\n",
    "    \n",
    "    # get true class\n",
    "    filenames = []\n",
    "    true_classes = []\n",
    "\n",
    "    for item in test_generator.filenames:\n",
    "        filenames.append(item)\n",
    "        # get true class from the filenames\n",
    "        true_class = item.split('\\\\')[0]\n",
    "        true_classes.append(true_class)\n",
    "    \n",
    "    return filenames, true_classes, pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精度算出関数\n",
    "def get_f1(true_labels_list: list,\n",
    "           predictions_list: list,\n",
    "           average_method: str,\n",
    "          ) -> (float, float, float):\n",
    "    \"\"\"This function will performs model inferencing using test data\n",
    "    and stores the results into the lists.\n",
    "    \n",
    "    Args:\n",
    "        true_labels_list (list): List of true labels.\n",
    "        predictions_list (list): List of predictions.\n",
    "        average_method (string): method to average score.\n",
    "        \n",
    "    Returns:\n",
    "        f1 (float): return f1 metric.\n",
    "        precision (float): return precision metric.\n",
    "        recall (float): return recall metric.\n",
    "    \"\"\"\n",
    "    f1 = f1_score(\n",
    "        y_true=true_labels_list,\n",
    "        y_pred=predictions_list,\n",
    "        average=average_method\n",
    "    )\n",
    "    \n",
    "    precision = precision_score(\n",
    "        y_true=true_labels_list,\n",
    "        y_pred=predictions_list,\n",
    "        average=average_method,\n",
    "    )\n",
    "    \n",
    "    recall = recall_score(\n",
    "        y_true=true_labels_list,\n",
    "        y_pred=predictions_list,\n",
    "        average=average_method,\n",
    "    )\n",
    "    \n",
    "    f1 = round(f1, 2)\n",
    "    precision = round(precision, 2)\n",
    "    recall = round(recall, 2)\n",
    "    \n",
    "    return f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 良否判定実行\n",
    "train_filenames, train_true_classes, train_pred_classes = get_predict(\n",
    "    model=model,\n",
    "    train_data_dir=train_data_dir,\n",
    "    test_data_dir=train_data_dir,\n",
    ")\n",
    "valid_filenames, valid_true_classes, valid_pred_classes = get_predict(\n",
    "    model=model,                                                        \n",
    "    train_data_dir=train_data_dir,                                                            \n",
    "    test_data_dir=validation_data_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.学習・検証データに対する精度評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精度算出\n",
    "train_f1, train_prec, train_recall = get_f1(\n",
    "    true_labels_list=train_true_classes,\n",
    "    predictions_list=train_pred_classes,\n",
    "    average_method='weighted',\n",
    ")\n",
    "valid_f1, valid_prec, valid_recall = get_f1(\n",
    "    true_labels_list=valid_true_classes,\n",
    "    predictions_list=valid_pred_classes,\n",
    "    average_method='weighted',\n",
    ")\n",
    "\n",
    "# 精度表示\n",
    "print('{:15}{:<15.2f}{:<15.2f}'.format('F1-score:', train_f1, valid_f1))\n",
    "print('{:15}{:<15.2f}{:<15.2f}'.format('Precision:', train_prec, valid_prec))\n",
    "print('{:15}{:<15.2f}{:<15.2f}'.format('Recall:', train_recall, valid_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.提出ファイルの出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータに対して良否判定を行い、その結果を提出フォーマットであるtsv形式で出力を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分類とラベルの対応確認\n",
    "label_map = (train_generator.class_indices)\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータに対して1つずつ予測し、テストデータのファイル名と判定結果をリストに保存\n",
    "file_list = []\n",
    "pred_list = []\n",
    "for file in glob.glob(test_data_dir + '/*'):\n",
    "    image_data = file\n",
    "    filename = file.split('/')[-1]\n",
    "    img = image.load_img(image_data, target_size=(IMG_WIDTH, IMG_HEIGHT))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x / 255\n",
    "    pred = model.predict(x)[0]\n",
    "    judge = np.argmax(pred)\n",
    "\n",
    "    # *bridge, horn, potatoを不良（'1'）に、regularを良品（'0'）に変換。if文の条件分岐は上の「分類とラベルの対応確認」セルの結果を参考に変更すること*\n",
    "    if judge==0:\n",
    "        judge=1\n",
    "    elif judge==1:\n",
    "        judge=1\n",
    "    elif judge==2:\n",
    "        judge=1\n",
    "    else:\n",
    "        judge=0\n",
    "\n",
    "    pred_list.append(judge)\n",
    "    file_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#判別結果をDataFrameに変換し、tsvファイルに出力\n",
    "df = pd.DataFrame([file_list, pred_list]).T\n",
    "submit = pd.read_csv(\"sample_submit.tsv\",sep=\"\\t\", header=None)\n",
    "submit[1] = df[1]\n",
    "submit.to_csv(\"my_submission.tsv\",\n",
    "              index=False,\n",
    "              header=False,\n",
    "              sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34e93c336985a5eb3bd805d3f1c0bc274ff44e546bf291a8de4c27c7c633b432"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
